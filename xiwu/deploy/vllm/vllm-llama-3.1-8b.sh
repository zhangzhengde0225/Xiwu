# Load and run the model:
vllm serve "meta-llama/Llama-3.1-8B-Instruct"